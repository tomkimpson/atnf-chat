# ATNF-Chat Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM API Configuration
# =============================================================================

# Anthropic API key (required for Claude)
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Model selection (optional, defaults shown)
ANTHROPIC_MODEL=claude-sonnet-4-20250514
# Alternative: claude-opus-4-20250514 for complex queries

# OpenRouter API key for free tier fallback (optional)
# Get a free key at: https://openrouter.ai/settings/keys
OPENROUTER_API_KEY=
OPENROUTER_MODEL=google/gemini-2.5-flash:free

# =============================================================================
# Application Settings
# =============================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# API server configuration
API_HOST=127.0.0.1
API_PORT=8000

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =============================================================================
# Catalogue Settings
# =============================================================================

# Path to cache catalogue data (optional)
# If not set, uses psrqpy default cache location
CATALOGUE_CACHE_DIR=

# Force catalogue refresh on startup (default: false)
CATALOGUE_FORCE_REFRESH=false

# =============================================================================
# Rate Limiting & Cost Control
# =============================================================================

# Maximum API calls per minute (optional)
MAX_API_CALLS_PER_MINUTE=60

# Maximum tokens per request (optional)
MAX_TOKENS_PER_REQUEST=4096

# =============================================================================
# Optional: Local Model Fallback (Ollama)
# =============================================================================

# Enable local model fallback when API unavailable
USE_LOCAL_FALLBACK=false

# Ollama server URL (if using local fallback)
OLLAMA_BASE_URL=http://localhost:11434

# Local model name
OLLAMA_MODEL=llama3.3:70b
