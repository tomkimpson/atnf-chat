# ATNF-Chat Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM API Configuration
# =============================================================================

# Anthropic API key for Claude
# Get your key at: https://console.anthropic.com/
#
# If set, users can chat without providing their own API key (server-side key).
# If not set, users must provide their own API key via the UI.
ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Model selection (optional, defaults shown)
ANTHROPIC_MODEL=claude-sonnet-4-20250514
# Alternative: claude-opus-4-20250514 for complex queries

# =============================================================================
# Application Settings
# =============================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# API server configuration
API_HOST=127.0.0.1
API_PORT=8000

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =============================================================================
# Catalogue Settings
# =============================================================================

# Path to cache catalogue data (optional)
# If not set, uses psrqpy default cache location
CATALOGUE_CACHE_DIR=

# Force catalogue refresh on startup (default: false)
CATALOGUE_FORCE_REFRESH=false

# =============================================================================
# Rate Limiting & Cost Control
# =============================================================================

# Rate limiting applies when using server-side API key (per-IP limiting).
# Users who provide their own API key bypass rate limiting.

# Maximum API calls per minute per IP (when using server key)
MAX_API_CALLS_PER_MINUTE=20

# Maximum tokens per request (optional)
MAX_TOKENS_PER_REQUEST=4096

# =============================================================================
# Optional: Local Model Fallback (Ollama)
# =============================================================================

# Enable local model fallback when API unavailable
USE_LOCAL_FALLBACK=false

# Ollama server URL (if using local fallback)
OLLAMA_BASE_URL=http://localhost:11434

# Local model name
OLLAMA_MODEL=llama3.3:70b
