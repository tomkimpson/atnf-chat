# ATNF-Chat Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM API Configuration — Three-Tier Hierarchy
# =============================================================================
#
# Tier 1 (Free Shared): Set OPENROUTER_API_KEY below. All visitors can chat
#   immediately using this shared key. This is the only required config.
#
# Tier 2 (User OpenRouter): Visitors can provide their own free OpenRouter key
#   (sk-or-...) in the browser for personal rate limits.
#
# Tier 3 (User Anthropic): Visitors can provide their own Anthropic key
#   (sk-ant-...) in the browser for the best quality experience.
#
# The user's key (if provided) always takes priority over the server keys.

# OpenRouter API key — powers the free shared tier for all visitors (required)
# Get a free key at: https://openrouter.ai/settings/keys
OPENROUTER_API_KEY=sk-or-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENROUTER_MODEL=google/gemini-2.5-flash

# Anthropic API key — server-side fallback (optional, used if no OpenRouter key)
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-sonnet-4-20250514
# Alternative: claude-opus-4-20250514 for complex queries

# =============================================================================
# Application Settings
# =============================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# API server configuration
API_HOST=127.0.0.1
API_PORT=8000

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =============================================================================
# Catalogue Settings
# =============================================================================

# Path to cache catalogue data (optional)
# If not set, uses psrqpy default cache location
CATALOGUE_CACHE_DIR=

# Force catalogue refresh on startup (default: false)
CATALOGUE_FORCE_REFRESH=false

# =============================================================================
# Rate Limiting & Cost Control
# =============================================================================

# Maximum API calls per minute (optional)
MAX_API_CALLS_PER_MINUTE=60

# Maximum tokens per request (optional)
MAX_TOKENS_PER_REQUEST=4096

# =============================================================================
# Optional: Local Model Fallback (Ollama)
# =============================================================================

# Enable local model fallback when API unavailable
USE_LOCAL_FALLBACK=false

# Ollama server URL (if using local fallback)
OLLAMA_BASE_URL=http://localhost:11434

# Local model name
OLLAMA_MODEL=llama3.3:70b
